}
}
origin_prob_table[1,]
origin_word_table[1,]
dum(origin_word_table=="penguin")
sum(origin_word_table=="penguin")
snow<-origin_word_table[5,]
snow
snow<-cbind(origin_word_table[5,],origin_prob_table[5,])
snow
snow<-rbind(origin_word_table[5,],origin_prob_table[5,])
?rbind
data.frame(words=origin_word_table[5,],probabilities=origin_prob_table[5,])
?data.frame
colnames(origin_prob_table)<-colnames(origin_word_table)
snow<-rbind(origin_word_table[5,],origin_prob_table[5,])
snow
write.csv(snow, "snow.csv")
snow
sum(origin_word_table=="ice")
sum(origin_prob_table==1)
sum(origin_prob_table==1, rm.na=TRUE)
sum(origin_prob_table==1, na.rm=TRUE)
max(origin_corpus_p_w)
max(origin_corpus_p_w, na.rm=TRUE)
max(p_t_and_w)
snow
(10/10000000)/(500/10000000)
(1/10000000)/(500/10000000)
0.02*1
0.02*0.049
0.02/0.00098
driveway10 <-(10/10000000)/(500/10000000)
driveway10 <-(10/10000000)/(500/10000000)*1
ice10<-(10/10000000)/(500/1000000)*0.049
driveway10/ice10
library(tm)
library(RTextTools)
library(tidyverse)
library(stringr)
library(lda)
library(textmineR)
library(stringi)
library(glmnet)
library(party)
library(qdap)
library(ridge)
######################  SESSION 1 create topic table   ########################
#read tables
statusswl <- read.csv("user_all_status_swl2.csv", header = T, fill=TRUE,row.names=NULL)
setwd("../../script/data/")
list.files('.')
statusswl <- read.csv("user_all_status_swl2.csv", header = T, fill=TRUE,row.names=NULL)
swl_time<- read.csv("time_status_swl.csv", header = T, fill=TRUE,row.names=NULL)
setwd("../")
list.files('.')
setwd("./data/")
swl<- read.csv("swl.csv", header = T, fill=TRUE,row.names=NULL)
#group status according to userid
statusswl2 <- aggregate(status~user_id, statusswl, FUN= paste, collapse=' ')
#clean data, replace smiley with words
smiles <- data.frame(s=c( ">:(", ":<", ">:", "=(", "=[", "='(",  "^_^",":)","=)","(=" ,"=]","^.^",":(",";)",";-)",":D","XD",":P",";P","<3"),
r=c("unhappyface","unhappyface","unhappyface","unhappyface","unhappyface","unhappyface","happyface","happyface","happyface","happyface","happyface","happyface","unhappyface","happyface","happyface","happyface","happyface","happyface","happyface","kiss"))
statusswl2$status %>%
str_replace_all(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ")%>%
str_replace_all("\\d", " ")%>%
stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)%>%
str_replace_all("[^[:alnum:]']", " ")%>%
str_replace_all('\\s\\b[^i]{1}\\s', " ")%>%
gsub("\\s+", " ",.) -> statusswl2$status
#create dtm
clean2.corpus <- Corpus(VectorSource(statusswl2$status))
dtm2 <- DocumentTermMatrix(clean2.corpus)
#convert dtm into matrix
freqs <- as.data.frame(as.matrix(dtm2))
#load topic list
setwd("../LDA topic words/")
topic2000 <- read.csv("2000_emo.csv", header = F, fill=F,row.names=NULL)
topic2001 <- t(topic2000)
#count word frequency according to the topic list, here shows how many topic words occur in each document
p4 <- vector()
for(i in 1:2000) {
o1 <- vector()
o1 <- freqs[c(1, match(topic2001[, i], names(freqs), nomatch=0))]
if(length(o1) > 2) {
topicCollumn = paste("topic", as.character(i), sep='')
p4[topicCollumn] <- as.data.frame(rowSums(o1))
}
}
p4[1:4]
topic[1,1:3]
p4[1,1:3]
o1
topic2001[1:5,3]
class(freqs)
names(freqs)
topic.table <- as.data.frame(p4)
topic.table[1:3,1:3]
sum(topic.table)
colSums(topic.table)
hist(colSums(topic.table))
hist(colSums(topic.table),n=100)
hist(rowSums(topic.table),n=100)
max(topic.table)
min(topic.table)
min(topic.table[topic.table>0])
?which
which(topic.table==1835,arr.ind=TRUE)
topic.table[1987,1898]
colnames(topic.table)[1898]
topic2001[1,1916]
topic2001[1:3,1916]
bob<-rowSums(o1)/rowSums(freqs)
user_word_count<-rowSums(freqs)
p4 <- vector()
for(i in 1:2000) {
o1 <- vector()
o1 <- freqs[c(1, match(topic2001[, i], names(freqs), nomatch=0))]
if(length(o1) > 2) {
topicCollumn = paste("topic", as.character(i), sep='')
p4[topicCollumn] <- as.data.frame(rowSums(o1)/user_word_count)
}
}
topic.table <- as.data.frame(p4)
topic.table[1,1:3]
topic.table[2,1:3]
topic.table[4,1:3]
max(topic.table)
0/0
topic.table[is.nan(topic.table)]<-0
topic.table[is.na(topic.table)]<-0
max(topic.table)
min(topic.table[topic.table>0])
topic.id <- as.data.frame(statusswl2[,1])
topic.id[2:1982] <- topic.table[1:1981]
colnames(topic.id)[1] <- "userid"
####################################### SESSION 2 TOPIC TABLE WITH LIWC ####################################
#count the number of words in each user
#topic.id$count <- sapply(strsplit(statusswl2$status, "\\s+"), length)
#remove users have less than 100 words
#topic.id2 <- topic.id[topic.id$count>100,]
#topic.id2 <- topic.id2[complete.cases(topic.id2), ]
#join topics with LIWC data
liwc <- read.csv("liwc.csv", header = T, fill=TRUE,row.names=NULL)
getwd()
setwd("../data/")
liwc <- read.csv("liwc.csv", header = T, fill=TRUE,row.names=NULL)
liwc2 <- liwc
liwc2[2:65] <- scale(liwc[,2:65])
######################## SESSION 3 SENTIMENT ####################################
# import positive and negative words
pos = readLines("positive_words.txt")
neg = readLines("negative_words.txt")
setwd("../sentiment_wordlist/")
pos = readLines("positive_words.txt")
neg = readLines("negative_words.txt")
score.sentiment = function(sentences, pos, neg, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos, neg) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos)
neg.matches = match(words, neg)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos, neg, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
###count the number of status in each user
detach("package:plyr", unload=TRUE)
statusswl %>%
group_by(user_id) %>%
mutate(count = n()) %>%
select(user_id,count)%>%
rename(userid = user_id) %>%
.[!duplicated(.$userid), ]-> status.count
summary(status.count$count)
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#1.0    36.0    98.0   142.3   201.0   500.0
sd(status.count$count)
#136.9811
hist(status.count$count)
###########clean data
statusswl$status %>%
#       as_vector() %>%
str_replace_all(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ")%>%
str_replace_all("\\d", " ")%>%
stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)%>%
str_replace_all("[^[:alnum:]']", " ")%>%
str_replace_all('\\s\\b[^i]{1}\\s', " ")%>%
gsub("\\s+", " ",.) -> statusswl$status
######compute sentiment score
result_all2 <- score.sentiment(statusswl$status, pos, neg)
statusswl.senti <- statusswl
statusswl.senti[3:4] <- result_all2[1:2]
colnames(statusswl.senti)[3] <- "sentiment.score"
colnames(statusswl.senti)[2] <- "userid"
#####join sentiment score with status count for each user
statusswl.senti %>%
select(userid, sentiment.score,text) %>%
left_join(status.count, by = "userid") %>%
right_join(swl, by = "userid") %>%
filter(count > 30) -> statusswl.senti2
statusswl.senti %>%
select(userid, sentiment.score,text) %>%
left_join(status.count, by = "userid") %>%
left_join(swl, by = "userid") %>%
filter(count > 30) -> statusswl.senti2
#mean sentiment score and swl score of each user + frequency of positive post and negative post
detach("package:plyr", unload=TRUE)
statusswl.senti2 %>% group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
#varible correlation with swl
cor(statusswl5$neg.freq,statusswl5$mean.swl)
#-0.2325164
statusswl5$pos.neg[!is.finite(statusswl5$pos.neg)] <- NA
cor(statusswl5$pos.neg,statusswl5$mean.swl,use = "complete.obs")
#0.1603037
cor.test(statusswl5$pos.freq,statusswl5$mean.swl)
#0.07833762
cor.test(statusswl5$mean.senti,statusswl5$mean.swl)
#0.2056476
#0.1810901 (original list)
#number of positive/negative score
sum(statusswl.senti2$sentiment.score > 0)
#184831
sum(statusswl.senti2$sentiment.score < 0)
#115915
sum(statusswl.senti2$sentiment.score == 0)
#194203
##correlation matrix of the sentiment variables, multicolinearity
statusswl5 %>%
select(mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)%>%
cor(.,use = "complete.obs") -> cor.matrix
########################  SESSION 4 JOIN ALL FEATURES  ########################################
#merge all features
statusswl5 %>%
select(userid,mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)  %>%
left_join(., topic.id, by = "userid") %>%
left_join(., liwc2, by = "userid") %>%
.[, colSums(is.na(.)) != nrow(.)] %>%
.[complete.cases(.), ] %>%
.[!duplicated(.$userid), ] -> allFeatures
###################################### SESSION 5 feature selection and prediction ########################
##2000 topics feature selection
###compute matrix rank
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = allFeatures$mean.swl
x = allFeatures[3:1988]
x <- as.matrix(x)
cv <- cv.glmnet(x,y,alpha=0.1)
coef(cv,s=0.1)
plot(cv)
cv$lambda.min
c <- coef(cv, s = "lambda.min")
#hand picked those == !0 as features
c
c[1]
c[2]
c[5]
length(c)
c.names
c.names()
names(c)
rownames(c)
sum(c[,1]>0)
cv_score<-as.matrix(unlist(c))
?"glmnet"
rownames(c[c!=0])
c[c!=0]
cvscore[cvscore!=0]
cv_score[cv_score!=0]
rownames(cv_score[cv_score!=0])
rownames(cv_score)
rownames(cv_score)[cv_score!=0]
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = trainset1$mean.swl
x = as.matrix(trainset1[3:1985])
cv1 <- cv.glmnet(x,y,alpha=1)
c <- coef(cv1, s = "lambda.min")
cv_score <- as.matrix(unlist(c))
testsetx <- as.matrix(testset1[3:1985])
pred2 <- predict(cv1,testsetx,s=c(0.1,0.05,0.01))
cor.test(pred2[,2], testset1$mean.swl)
#0.2517533
cor(allFeatures$sentiment,allFeatures$swl)
cor(allFeatures$mean.senti,allFeatures$mean.swl)
set.seed(66236)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
set.seed(23778672)
lassonames<-rownames(cv_score)[cv_score!=0]
lassonames
lassonames<-lassonames[-"(Intercept)"]
lassonames<-lassonames[-c("(Intercept)")]
lassonames<-lassonames[lassonames!="(Intercept)"]
lassonames
set.seed(23778672)
fit1<-cforest(mean.swl~lassonames,data=trainset1,controls=cforest_unbiased(ntree=1000,mtry=3))
fit1<-cforest(as.formula(paste("mean.swl~",lassonames,collapse="+"),data=trainset1,controls=cforest_unbiased(ntree=1000,mtry=3))
)
fit1<-cforest(as.formula(paste("mean.swl~",lassonames,collapse="+")),data=trainset1,controls=cforest_unbiased(ntree=1000,mtry=3))
paste("mean.swl~",lassonames,collapse="+")
paste("mean.swl~",paste(lassonames,collapse="+"))
as.formula(paste("mean.swl~",paste(lassonames,collapse="+")))
fit1<-cforest(as.formula(paste("mean.swl~",paste(lassonames,collapse="+"))), data=trainset1,controls=cforest_unbiased(ntree=1000,mtry=3))
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
cor(testset1$Prediction1,testset1$mean.swl)
allnames<-paste0(lassonames, "+we+article+work+home+leisure+number+discrep+anger+negemo+friend+negate+swear+hear")
allnames
as.formula(paste0(paste("mean.swl~",paste(lassonames,collapse="+")),"+we+article+work+home+leisure+number+discrep+anger+negemo+friend+negate+swear+hear"))
allformula<-as.formula(paste0(paste("mean.swl~",paste(lassonames,collapse="+")),"+we+article+work+home+leisure+number+discrep+anger+negemo+friend+negate+swear+hear"))
fit1<-cforest(allformula,data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
?predict
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
testset1
cor(testset1$Prediction1,testset1$mean.swl)
p4 <- vector()
for(i in 1:2000) {
o1 <- vector()
o1 <- freqs[c(1, match(topic2001[, i], names(freqs), nomatch=0))]
if(length(o1) > 2) {
topicCollumn = paste("topic", as.character(i), sep='')
p4[topicCollumn] <- as.data.frame(rowSums(o1))
}
}
topic.table <- as.data.frame(p4)
topic.table[1:3,1:3]
topic.id <- as.data.frame(statusswl2[,1])
topic.id[1,1]
statusswl2[1,1:5]
topic.id[1,1:5]
topic.id[1,1:2]
topic.id[1,1]
topic.id[2:1982] <- topic.table[1:1981]
topic.id[1,:3]
topic.id[1,1:3]
statusswl2$status[1]
topic.id[1,1:5]
statusswl2[1,]
topic2001[1,]
topic2001[,1]
freqs
bob<-unique(topic2001)
dim(bob)
bob<-unique(flatten(topic2001))
class(topic2001)
bob<-unique(flatten(topic2000))
length(bob)
bob[1:3]
length(unlist(bob))
unlist(bob)[1:5]
topic2001[,1]
topic2000[1,]
bob<-unique(unlist(topic2000))
dim(bob)
length(bob)
dtm3<-dtm2
dtm3<-dtm3[,bob]
dtm3[1:3,3]
dtm3<-NULL
delete(dtm3)
rm(dtm3)
freqs2<-as.data.frame(as.matrix(dtm2))
freqs2<-freqs2[,bob]
freqs<-freqs2
user_word_count<-rowSums(freqs)
#load topic list
topic2000 <- read.csv("2000_emo.csv", header = F, fill=F,row.names=NULL)
topic2001 <- t(topic2000)
#count word frequency according to the topic list, here shows how many topic words occur in each document
p4 <- vector()
for(i in 1:2000) {
o1 <- vector()
o1 <- freqs[c(1, match(topic2001[, i], names(freqs), nomatch=0))]
if(length(o1) > 2) {
topicCollumn = paste("topic", as.character(i), sep='')
p4[topicCollumn] <- as.data.frame(rowSums(o1))
}
}
topic.table <- as.data.frame(p4)
#add userid to p4
topic.id <- as.data.frame(statusswl2[,1])
topic.id[2:1982] <- topic.table[1:1981]
colnames(topic.id)[1] <- "userid"
topic.id[1,1:4]
topic.table <- as.data.frame(p4)
#add userid to p4
topic.id <- as.data.frame(statusswl2[,1])
topic.id[2:1982] <- topic.table[1:1981]
topic.id[2:ncol(topic.table)+1] <- topic.table[1:ncol(topic.table)]
bob<-cbind(topic.id,topic.table)
topic.id<-cbind(topic.id,topic.table)
colnames(topic.id)[1] <- "userid"
topic.id[1,4]
topic.id[1,1:4]
topic.id[2,1:4]
topic.id[3,1:4]
max(topic.id[1,])
max(topic.id[1,2:])
max(topic.id[1,2:ncol(topic.id)])
topic2001 %>%
stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE) ->topic2002
bob<-unique(topic2002)
bob<-apply(topic2002,2,unique)
bob<-apply(topic2001,2,unique)
bob[1]
bob[2]
bob<-apply(topic2001,2,function(x){stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)%>%})
bob<-apply(topic2001,2,function(x){stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)})
bob<-apply(topic2001,2,function(x,smile){stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)})
bob<-apply(topic2001,2,function(x,smiles){stri_replace_all_fixed(pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)},smiles)
bob<-apply(topic2001,2,function(x,smiles){stri_replace_all_fixed(x,pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)},smiles)
dim(bob)
bob[,1]
bob[,2]
bob<-as.matrix(apply(bob,2,unique))
dim(bob)
bob[1]
topic2001[1:3,1:3]
dave<-as.matrix(unlist(bob))
dim(dave)
listHolder<-bob
dd  <-  as.data.frame(matrix(unlist(listHolder), nrow=length(unlist(listHolder[1]))))
clean2.corpus <- Corpus(VectorSource(statusswl2$status))
dtm2 <- DocumentTermMatrix(clean2.corpus)
#convert dtm into matrix
freqs <- as.data.frame(as.matrix(dtm2))
#user_word_count<-rowSums(freqs)
#load topic list
topic2000 <- read.csv("2000_emo.csv", header = F, fill=F,row.names=NULL)
topic2001 <- t(topic2000)
topic2001 <- t(topic2000)
topic2000 <- read.csv("2000_emo.csv", header = F, fill=F,row.names=NULL)
topic_corpus<-unique(unlist(topic2000))
topic_corpus<-unique(topic2000)
topic_corpus<-as.character(unique(unlist(topic2000)))
topic_corpus[1:30]
length(unique(topic_corpus))
20*2000
dtm<-dtm2[,topic_corpus]
freqs2<freqs[,topic_corpus]
dim(freqs)
freqs <- as.data.frame(as.matrix(dtm2))
freqs2<freqs[,topic_corpus]
freqs2<freqs[,topic_corpus %in% colnames(freqs)]
rm(freqs2)
freqs2<freqs[,topic_corpus]
freqs2<-freqs[,topic_corpus]
class(topic_corpus)
freqs2<-freqs[,as.factor(topic_corpus)]
freqs2<-freqs[,as.factor(topic_corpus)]
p4 <- vector()
for(i in 1:2000) {
o1 <- vector()
o1 <- freqs2[c(1, match(topic2001[, i], names(freqs2), nomatch=0))]
if(length(o1) > 2) {
topicCollumn = paste("topic", as.character(i), sep='')
p4[topicCollumn] <- as.data.frame(rowSums(o1))
}
}
topic.table <- as.data.frame(p4)
#add userid to p4
topic.id <- as.data.frame(statusswl2[,1])
#add userid to the topic score table
topic.id <- as.data.frame(statusswl2[,1])
topic.id<-cbind(topic.id,topic.table)
colnames(topic.id)[1] <- "userid"
topic.id[1:3,1:3]
sum(colsum(topic.id)==0)
sum(colSums(topic.id)==0)
colsums(topic.id)
colSums(topic.id)
topic.id[is.na(topic.id)]=0
sum(colSums(topic.id)==0)
bob<-apply(topic.id,2,sum)
is.character((topic.id))
o1
is.character(rowSums(01))
is.character(rowSums(o1))
