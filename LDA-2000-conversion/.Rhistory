colnames(statusswl1.senti)[3] <- "sentiment.score"
colnames(statusswl1.senti)
colnames(statusswl1.senti)[1] <- "userid"
#####join sentiment score with status count for each user
#do once to study the number of posts per user for the one month preceding SWL survey
swl_unq<-swl[!duplicated(swl$user),]
statusswl1.senti %>%
select(userid, sentiment.score,text) %>%
left_join(status.count, by = "userid") %>%
left_join(swl_unq, by = "userid") -> statusswl1.senti2_all
colnames(statusswl1.senti2_all)
statusswl1.senti2_all[1,]
hist(statusswl.senti2_all$count)
hist(statusswl1.senti2_all$count)
hist(statusswl1.senti2_all$count,100)
statusswl1.senti2_all %>%
filter(count > 10) -> statusswl1.senti2_filt
View(statusswl1.senti2_filt)
statusswl1.senti2_filt %>% group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
statusswl1.senti2_filt %>%
group_by(., userid) -> tmp1
colnames(statusswl1.senti2_filt)
length(unique(statusswl1.senti2_filt$userid))
colnames(tmp1)
?group_by
head(tmp1)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) ->tmp1
head(tmp1)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) ->tmp1
head(tmp1)
head(tmp1[,4:8])
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
statusswl5
?summarise
detach("package:plyr", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
library("plyr")
?summarise
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
library(dplyr)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
detach(package:plyr)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
ddply(summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
cor.test(statusswl5$neg.freq,statusswl5$mean.swl)
statusswl5$pos.neg[!is.finite(statusswl5$pos.neg)]
statusswl5$pos.neg[!is.finite(statusswl5$pos.neg)] <- NA
statusswl5$pos.neg[!is.finite(statusswl5$pos.neg)]
cor.test(statusswl5$pos.neg,statusswl5$mean.swl,use = "complete.obs")
cor.test(statusswl5$pos.freq,statusswl5$mean.swl)
cor.test(statusswl5$mean.senti,statusswl5$mean.swl)
sum(statusswl1.senti2_filt$sentiment.score > 0)
sum(statusswl1.senti2_filt$sentiment.score < 0)
sum(statusswl1.senti2_filt$sentiment.score == 0)
statusswl5 %>%
select(mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)%>%
cor(.,use = "complete.obs") -> cor.matrix
cor.matrix
statusswl5 %>%
select(mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)%>%
cor.test(.,use = "complete.obs") -> cor.matrix
statusswl5 %>%
select(mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)%>%
cor.test(.,use = "complete.obs") -> cor.matrix
?cor
statusswl5 %>%
select(userid,mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)  %>%
left_join(., topic.id, by = "userid") %>%
left_join(., liwc2, by = "userid") %>%
.[, colSums(is.na(.)) != nrow(.)] %>%
.[complete.cases(.), ] %>%
.[!duplicated(.$userid), ] -> allFeatures
###################################### SESSION 5 feature selection and prediction ########################
##2000 topics feature selection
###compute matrix rank
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = allFeatures$mean.swl
x = allFeatures[3:1988]
x <- as.matrix(x)
cv <- cv.glmnet(x,y,alpha=0.1)
cv
coef(cv,s=0.1)
plot(cv)
cv$lambda.min
c <- coef(cv, s = "lambda.min")
#hand picked those == !0 as features
cv_score <- as.matrix(unlist(c))
#lasso prediction using lda topics
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = trainset1$mean.swl
x = as.matrix(trainset1[3:1985])
cv1 <- cv.glmnet(x,y,alpha=1)
c <- coef(cv1, s = "lambda.min")
cv_score <- as.matrix(unlist(c))
testsetx <- as.matrix(testset1[3:1985])
pred2 <- predict(cv1,testsetx,s=c(0.1,0.05,0.01))
cor.test(pred2[,2], testset1$mean.swl)
pred2
?predict
?cv.glmnet
install.packages("Metrics")
library(Metrics)
rmse(testset1$mean.swl,pred2[,2])
?rmse
auc(testset1$mean.swl,pred2[,2])
sum(is.na(testset1$mean.swl))
sum(is.na(pred2[,2]))
?auc
rmse(testset1$mean.swl,pred2[,2])
max(t5$mean.swl)
min(t5$mean.swl)
vc1
cv1
cv1$cvm
cv1$name
rmse(testset1$mean.swl,pred2[,2])
min(t5$mean.swl)
max(t5$mean.swl)
cor.matrix
cor(allFeatures$sentiment,allFeatures$swl)
colnames(allFeatures)
cor(allFeatures$mean.senti,allFeatures$mean.swl)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
set.seed(23778672)
fit1 <- cforest(mean.swl ~ mean.senti+pos.neg+neg.freq+topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252+we+article+work+home+leisure+number+discrep+anger+negemo+friend+negate+swear+hear
,data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl, testset1$Prediction1)
set.seed(6634258)
fit2 <- cforest(mean.swl ~ funct+pronoun+ppron+i+we+you+shehe+they+ipron+article+verb
+auxverb+past+present+future+adverb+preps+conj+negate+quant+number+swear+social
+family+friend+humans+affect+posemo+negemo+anx+anger+sad+cogmech+insight+cause
+discrep+tentat+certain+inhib+incl+excl+percept+see+hear+feel+bio+body+health
+sexual+ingest+relativ+motion+space+time+work+achieve+leisure+home+money+relig
+death+assent+nonfl+filler,
data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 2))
testset1$Prediction2 <- predict(fit2, testset1, OOB=TRUE)
cor.test(testset1$Prediction2,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction2)
set.seed(2042237)
fit3 <- cforest(mean.swl ~ topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252,
data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit3, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction1)
set.seed(20247)
fit1 <- cforest(mean.swl ~ mean.senti+pos.neg+neg.freq+topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252,data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction1)
statusswl1.senti2_all %>%
filter(count > 20) -> statusswl1.senti2_filt
#mean sentiment score and swl score of each user + frequency of positive post and negative post
detach("package:plyr", unload=TRUE)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) %>%
mutate(pos.freq = sum(sentiment.score > 0)/count) %>%
mutate(pos.neg = as.numeric(sum(sentiment.score > 0)/sum(sentiment.score < 0))) %>%
summarise(., mean.senti=mean(sentiment.score),mean.swl=mean(swl),pos.freq=mean(pos.freq),neg.freq=mean(neg.freq),pos.neg=mean(pos.neg))-> statusswl5
#variable correlation with swl
cor.test(statusswl5$neg.freq,statusswl5$mean.swl)
statusswl5$pos.neg[!is.finite(statusswl5$pos.neg)] <- NA
cor.test(statusswl5$pos.neg,statusswl5$mean.swl,use = "complete.obs")
cor.test(statusswl5$pos.freq,statusswl5$mean.swl)
cor.test(statusswl5$mean.senti,statusswl5$mean.swl)
sum(statusswl1.senti2_filt$sentiment.score > 0)
sum(statusswl1.senti2_filt$sentiment.score < 0)
sum(statusswl1.senti2_filt$sentiment.score < 0)
8608/6364
5787/4375
sum(statusswl1.senti2_filt$sentiment.score == 0)
5787/(8608+6364+11691)
4375/(8608+6364+11691)
8608/(8608+6364+11691)
6364/(8608+6364+11691)
5787/(5787+4375+8039)
4375/(5787+4375+8039)
statusswl5 %>%
select(mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)%>%
cor(.,use = "complete.obs") -> cor.matrix
cor.matrix
statusswl5 %>%
select(userid,mean.swl,mean.senti,pos.freq,neg.freq,pos.neg)  %>%
left_join(., topic.id, by = "userid") %>%
left_join(., liwc2, by = "userid") %>%
.[, colSums(is.na(.)) != nrow(.)] %>%
.[complete.cases(.), ] %>%
.[!duplicated(.$userid), ] -> allFeatures
###################################### SESSION 5 feature selection and prediction ########################
##2000 topics feature selection
###compute matrix rank
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = allFeatures$mean.swl
x = allFeatures[3:1988]
x <- as.matrix(x)
cv <- cv.glmnet(x,y,alpha=0.1)
coef(cv,s=0.1)
plot(cv)
cv$lambda.min
c <- coef(cv, s = "lambda.min")
#hand picked those == !0 as features
cv_score <- as.matrix(unlist(c))
head(cv_score)
nrow(cv_score)
head(c)
head(cv_score)
cv_score[1:5,1]
cv_score.values
cv_score["mean.senti"]
cv_score[1]
sum(cv_score!=0)
sum(cv_score==0)
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = allFeatures$mean.swl
x = allFeatures[3:1988]
x <- as.matrix(x)
cv <- cv.glmnet(x,y,alpha=0.1)
coef(cv,s=0.1)
plot(cv)
cv$lambda.min
c <- coef(cv, s = "lambda.min")
#hand picked those == !0 as features
cv_score <- as.matrix(unlist(c))
#lasso prediction using lda topics
set.seed(666)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
y = trainset1$mean.swl
x = as.matrix(trainset1[3:1985])
colnames(trainset[1,1:3])
colnames(trainset1[1,1:3])
cv1 <- cv.glmnet(x,y,alpha=1)
c <- coef(cv1, s = "lambda.min")
cv_score <- as.matrix(unlist(c))
testsetx <- as.matrix(testset1[3:1985])
pred2 <- predict(cv1,testsetx,s=c(0.1,0.05,0.01))
cor.test(pred2[,2], testset1$mean.swl)
rmse(testset1$mean.swl,pred2[,2])
cor(allFeatures$mean.senti,allFeatures$mean.swl)
set.seed(66236)
t5 <- allFeatures
ind = sample(2, nrow(t5), replace = TRUE, prob=c(0.7, 0.3))
trainset1 = t5[ind == 1,]
testset1 = t5[ind == 2,]
set.seed(23778672)
fit1 <- cforest(mean.swl ~ mean.senti+pos.neg+neg.freq+topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252+we+article+work+home+leisure+number+discrep+anger+negemo+friend+negate+swear+hear
,data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl, testset1$Prediction1)
set.seed(6634258)
fit2 <- cforest(mean.swl ~ funct+pronoun+ppron+i+we+you+shehe+they+ipron+article+verb
+auxverb+past+present+future+adverb+preps+conj+negate+quant+number+swear+social
+family+friend+humans+affect+posemo+negemo+anx+anger+sad+cogmech+insight+cause
+discrep+tentat+certain+inhib+incl+excl+percept+see+hear+feel+bio+body+health
+sexual+ingest+relativ+motion+space+time+work+achieve+leisure+home+money+relig
+death+assent+nonfl+filler,
data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 2))
testset1$Prediction2 <- predict(fit2, testset1, OOB=TRUE)
cor.test(testset1$Prediction2,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction2)
set.seed(2042237)
fit3 <- cforest(mean.swl ~ topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252,
data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit3, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction1)
set.seed(20247)
fit1 <- cforest(mean.swl ~ mean.senti+pos.neg+neg.freq+topic588+topic1179+topic205+topic1411+topic700+topic605+topic48
+topic555+topic939+topic855+topic253+topic1715+topic675+topic1440+topic1725+topic638
+topic171+topic327+topic217+topic1530+topic51+topic1221+topic470+topic505+topic1819+topic379
+topic1684+topic314+topic754+topic1873+topic38+topic15+topic126+topic815+topic1620+topic111
+topic1808+topic1038+topic994+topic321+topic846+topic765+topic259+topic1316+topic1980
+topic1573+topic812+topic1139+topic551+topic1964+topic1079+topic1590+topic1186+topic1095
+topic1680+topic630+topic409+topic677+topic53+topic1801+topic479+topic1098+topic142+topic938
+topic863+topic436+topic539+topic1041+topic1508+topic1543+topic950+topic3+topic1787+topic1540
+topic1549+topic1171+topic603+topic179+topic1772+topic820+topic1235+topic1043+topic120+topic1625
+topic610+topic1689+topic811+topic1938+topic425+topic1745+topic867+topic1376+topic23+topic1448
+topic428+topic1575+topic667+topic1708+topic1478+topic150+topic704+topic1423+topic961+topic592
+topic427+topic162+topic450+topic956+topic1113+topic569+topic1814+topic1010+topic353+topic1150
+topic524+topic249+topic1886+topic22+topic90+topic455+topic823+topic708+topic1390+topic530
+topic1252,data = trainset1,controls=cforest_unbiased(ntree=1000, mtry= 3))
testset1$Prediction1 <- predict(fit1, testset1, OOB=TRUE)
cor.test(testset1$Prediction1,testset1$mean.swl)
rmse(testset1$mean.swl,testset1$Prediction1)
statusswl1.senti2_filt %>%
group_by(., userid) %>%
mutate(neg.freq = sum(sentiment.score < 0)/count) -> tmp1
head(tmp1)
tmp1[tmp1$userid==tmp1$userid[1],c("sentiment.score","neg.freq")]
?laply
setwd('~/Dropbox/myPersonality/submit_version/PLOS ONE/revision/script/LDA-2000-conversion/')
origin<-read.csv('2000topics.top20freqs.keys.csv',header = F, fill=FALSE,row.names=NULL, stringsAsFactors = FALSE)
origin<-origin[,-1]
range(1,10)
seq(1,10)
seq(1,10,2)
word_table<-origin[,seq(1,ncol(origin),2)]
head(word_table)
seq(2,10,2)
freq_table<-origing[,seq(2,ncol(origin),2)]
freq_table<-origin[,seq(2,ncol(origin),2)]
total_words<-sum(sum(freq_table))
?sum
total_words<-sum(sum(freq_table, na.rm=TRUE))
total_words<-sum(sum(freq_table, na.rm=TRUE), na.rm=TRUE)
total_words
flatten(word_table)
length(unique(flatten(word_table)))
origin_word_table<-origin[,seq(1,ncol(origin),2)]
origin_freq_table<-origin[,seq(2,ncol(origin),2)]
origin_total_words<-sum(sum(freq_table, na.rm=TRUE), na.rm=TRUE)
origin_corpus<-unique(flatten(origin_word_table))
origin_freq_table[origin_word_table==origin_corpus[1]]
origin_freq_table[origin_word_table==origin_corpus[2]]
origin_corpus[1]
origin_corpus[2]
sum(origin_freq_table[origin_word_table==origin_corpus[2]])
for i in seq(1,10){
for (i in seq(1,10)){
i}
bob<-integer(3)
bob
origin_corpus<-unique(flatten(origin_word_table))
origin_corpus_freqs<-integer(length(origin_corpus))
for (i in seq(1,length(origin_corpus))){
origin_corpus_freqs[i]<-sum(origin_freq_table[origin_word_table==origin_corpus[i]])
}
)
is.na(origin_freq_table)
origin_corpus_p_w<-origin_corpus_freqs/origin_total_words
origin_corpus_p_w[1:5]
for (topic_index in seq(1,5){
for (topic_index in seq(1,5)){
topic<-origin_word_table[topic_index,]
words<-topic[!is.na(topic)]
}
words
origin_prob_table<-origin_freq_table
#to calculate p(topic|word), we can loop through the word table, and use the word to search the corpus probabilities list - p(word), and use the loop indexes to select the word frequency - to create p(topic and word)
for (topic_index in seq(1,nrow(origin_word_table)){
topic<-origin_word_table[topic_index,]
words<-topic[!is.na(topic)]
for (word_index in seq(1,length(words))){
origin_prob_table<-origin_freq_table
#to calculate p(topic|word), we can loop through the word table, and use the word to search the corpus probabilities list - p(word), and use the loop indexes to select the word frequency - to create p(topic and word)
for (topic_index in seq(1,nrow(origin_word_table))){
topic<-origin_word_table[topic_index,]
words<-topic[!is.na(topic)]
for (word_index in seq(1,length(words))){
freq<-origin_freq_table[topic_index,word_index] #obtain the frequency for this word in this topic
p_t_and_w<-freq/origin_total_words #compute p(topic and word)
word<-words[word_index] #get the actual word
p_w<-origin_corpus_p_w[origin_corpus==word] #use the word to look up the p(word) computed earlier
p_t_w<-p_t_and_w/p_w # compute the p(topic|word) as p(topic and word)/p(word)
origin_prob_table[topic_index, word_index]<-p_t_w #store the result
}
}
origin_prob_table[1:5,1:5]
origin_word_table[1:5,1:5]
origin_word_table[5,]
origin_prob_table[5,]
sum(origin_word_table=="driveway")
freqs[1,1]
freqs <- as.data.frame(as.matrix(dtm2))
freqs[1:2,1:3]
max(freqs[,1])
topic2000[1:2,1:3]
colnames(liwv)
colnames(liwc)
?rmse()
p_t_w[1,]
p_t_w
origin_prob_table
sum(origin_prob_table==1)
sum(origin_prob_table==1, rm.na=TRUE)
?sum
sum(origin_prob_table==1, na.rm=TRUE)
which(origin_prob_table == "M017"), arr.ind=TRUE)
apply(origin_prob_table, 1, function(r) any(r %in% c(1)))
sum(apply(origin_prob_table, 1, function(r) any(r %in% c(1))))
origin_prob_table[1,]
origin_word_table[1,]
origin_word_table[2,]
origin_word_table[3,]
origin_word_table[4,]
origin_word_table[5,]
origin_prob_table[5,]
